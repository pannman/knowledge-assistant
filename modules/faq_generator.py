"""
FAQç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Chain of Thoughtã€Few-shotã¨Structured Outputã‚’æ´»ç”¨ã—ãŸæ”¹è‰¯ç‰ˆFAQç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
OpenAI APIã‚’ç›´æ¥ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³
"""

import os
from openai import OpenAI  # OpenAI APIã‚’ç›´æ¥ä½¿ç”¨
from typing import List, Dict, Any, Tuple
import tiktoken  # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆç”¨
from tqdm import tqdm  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¡¨ç¤ºç”¨

from .text_processors import PDFTextProcessor, SlackConversationProcessor
from .prompt_templates import PromptBuilder
from .postprocessors import FAQPostProcessor, LLMOutputParser


class FAQGenerator:
    """LLMã‚’ä½¿ç”¨ã—ã¦FAQã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    
    def __init__(self):
        """FAQç”Ÿæˆå™¨ã®åˆæœŸåŒ–"""
        # OpenAI APIç›´æ¥ä½¿ç”¨
        self.client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.model = "gpt-4"
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼ã®åˆæœŸåŒ–
        self.prompt_builder = PromptBuilder()
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼åˆæœŸåŒ–
        self.encoding = tiktoken.encoding_for_model(self.model)
        self.total_tokens_used = 0
    
    def count_tokens(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        return len(self.encoding.encode(text))
    
    def generate_summary(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®è¦ç´„ã‚’ç”Ÿæˆ"""
        try:
            print("\nğŸ“ è¦ç´„ã‚’ç”Ÿæˆä¸­...")
            input_tokens = self.count_tokens(text)
            print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {input_tokens}")

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ç°¡æ½”ã«è¦ç´„ã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": f"ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’200æ–‡å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n\n{text}"}
                ],
                max_tokens=300,
                temperature=0.3
            )
            
            output = response.choices[0].message.content.strip()
            output_tokens = self.count_tokens(output)
            total_tokens = response.usage.total_tokens
            self.total_tokens_used += total_tokens
            
            print(f"ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {output_tokens}")
            print(f"ã“ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: {total_tokens}")
            print(f"ç´¯è¨ˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: {self.total_tokens_used}")
            
            return output
        
        except Exception as e:
            print(f"âŒ è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return "è¦ç´„ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    def generate_faqs_from_text(
        self, 
        text: str, 
        source_type: str, 
        source_id: str, 
        source_url: str, 
        context: str = ""
    ) -> List[Dict[str, Any]]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰FAQã‚’ç”Ÿæˆ"""
        try:
            print("\nğŸ”„ FAQç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹...")
            
            # 1. ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
            print("1ï¸âƒ£ ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç†ä¸­...")
            if source_type == "pdf":
                processed_result = PDFTextProcessor.process(text)
                processed_text = processed_result["enhanced_text"]
            elif source_type == "slack":
                processed_result = SlackConversationProcessor.process(text)
                processed_text = processed_result["enhanced_text"]
            else:
                processed_text = text
            
            # 2. ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            print("2ï¸âƒ£ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆä¸­...")
            if source_type == "pdf":
                prompt = self.prompt_builder.get_pdf_faq_prompt(processed_text, context)
            elif source_type == "slack":
                prompt = self.prompt_builder.get_slack_faq_prompt(processed_text, context)
            else:
                prompt = self.prompt_builder.get_generic_faq_prompt(processed_text, context)
            
            input_tokens = self.count_tokens(prompt)
            print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {input_tokens}")
            
            # 3. LLMã‚’ä½¿ç”¨ã—ã¦FAQã‚’ç”Ÿæˆ - OpenAI APIç›´æ¥ä½¿ç”¨
            print("3ï¸âƒ£ LLMã§FAQã‚’ç”Ÿæˆä¸­...")
            response = self.client.chat.completions.create(
                model=self.model,  # "gpt-4o"ãªã©ã®OpenAIãƒ¢ãƒ‡ãƒ«å
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯é«˜å“è³ªãªFAQç”ŸæˆAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=3000,  # Chain of Thoughtã®ãŸã‚å¢—é‡
                temperature=0.5
            )
            
            llm_response = response.choices[0].message.content.strip()
            output_tokens = self.count_tokens(llm_response)
            total_tokens = response.usage.total_tokens
            self.total_tokens_used += total_tokens
            
            print(f"ç”Ÿæˆã•ã‚ŒãŸFAQã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {output_tokens}")
            print(f"ã“ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: {total_tokens}")
            print(f"ç´¯è¨ˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: {self.total_tokens_used}")
            
            # 4. LLMå‡ºåŠ›ã‚’è§£æ
            print("4ï¸âƒ£ LLMå‡ºåŠ›ã‚’è§£æä¸­...")
            faqs = LLMOutputParser.parse_faq_response(llm_response)
            
            # FAQã®è§£æã¨è¡¨ç¤ºã‚’è¿½åŠ 
            print("\nğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸFAQä¸€è¦§:")
            for i, faq in enumerate(faqs, 1):
                print(f"\n----- FAQ #{i} -----")
                print(f"Q: {faq.get('question', '')}")
                print(f"A: {faq.get('answer', '')}")
                print("-" * 50)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
            print("\n5ï¸âƒ£ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ä¸­...")
            result = []
            for faq in faqs:
                result.append({
                    "question": faq.get("question", ""),
                    "answer": faq.get("answer", ""),
                    "source_type": source_type,
                    "source_id": source_id,
                    "source_url": source_url
                })
            
            # FAQå¾Œå‡¦ç†
            print("6ï¸âƒ£ FAQå¾Œå‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
            processed_faqs = FAQPostProcessor.process(result, source_type, source_url)
            
            # å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            print(f"\nâœ… FAQç”Ÿæˆå®Œäº†ï¼")
            print(f"â€¢ ç”Ÿæˆã•ã‚ŒãŸFAQæ•°: {len(processed_faqs)}")
            print(f"â€¢ ç´¯è¨ˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: {self.total_tokens_used}")
            
            return processed_faqs
        
        except Exception as e:
            print(f"âŒ FAQç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return []